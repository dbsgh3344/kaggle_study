{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"week5_algorithm.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN/CM7LaDkvqfO7l/BjmSKS"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ersig9KnsgNK","colab_type":"text"},"source":["# Decision Tree\n","\n","의사결정나무는 데이터를 분석하여 이들 사이에 존재하는 패턴을 예측 가능한 규칙들의 조합으로 나타내며, 그 모양이 ‘나무’와 같다고 해서 의사결정나무라 불립니다. 질문을 던져서 대상을 좁혀나가는 ‘스무고개’ 놀이와 비슷한 개념입니다. \n","\n","<img src= \"https://miro.medium.com/max/1430/1*rSQIIAboJftqAv_BReNipg.png\" width=\"70%\">"]},{"cell_type":"markdown","metadata":{"id":"3GmP_bEzuGfo","colab_type":"text"},"source":["depth가 깊어질수록 데이터의 개수는 줄어듭니다. 각 terminal node에 속하는 데이터의 개수를 합하면 root node의 데이터수와 일치합니다."]},{"cell_type":"markdown","metadata":{"id":"0hLFyPRM1fgx","colab_type":"text"},"source":["불순도(Impurity)란 해당 범주 안에 서로 다른 데이터가 얼마나 섞여 있는지를 뜻합니다. 아래 그림에서 위쪽 범주는 불순도가 낮고, 아래쪽 범주는 불순도가 높습니다. "]},{"cell_type":"markdown","metadata":{"id":"rAiDNK3KUDNF","colab_type":"text"},"source":["## Entropy  \n","불확실성을 측정하기 위한 척도.  \n","<img src= \"https://gaussian37.github.io/assets/img/ml/concept/Information-Theory/Cross-Entropy_print.png\" width=\"50%\"> \n","\n","k: 클래스 개수  \n","p : k 클래스에 속하는 관찰값의 비율\n","\n",". "]},{"cell_type":"markdown","metadata":{"id":"i1pMPQMxVjr0","colab_type":"text"},"source":["<img src = \"http://i.imgur.com/heIgkif.png\" width=\"25%\">\n","\n","k = 2 (빨강,파랑)  \n","\n","빨간 점선이 없고 주황색 영역만 존재할 때, 이 영역에서의 불확실성은   \n","D =  0.9544 \n"]},{"cell_type":"code","metadata":{"id":"se5fUQbEblgB","colab_type":"code","outputId":"edce2d72-960e-40b9-e814-fe73a002ae20","executionInfo":{"status":"ok","timestamp":1588394812609,"user_tz":-540,"elapsed":1803,"user":{"displayName":"Mike Pk","photoUrl":"","userId":"01708764291721906768"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import numpy as np\n","H = (-(10/16) *np.log2(10/16)) + (-(6/16) * np.log2(6/16))\n","H"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9544340029249649"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"17kIvqAltDEZ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uTIyvehBcCaV","colab_type":"text"},"source":["## conditional Entropy\n","\n","\n","<img src = \"https://www.researchgate.net/profile/David_Mccaulay2/publication/280092310/figure/fig3/AS:667668197670916@1536195911305/Entropy-and-conditional-entropy-formulas.png\" width =\"50%\">"]},{"cell_type":"markdown","metadata":{"id":"vhiJOMdcdvfM","colab_type":"text"},"source":["빨간 점선이 주어졌을 때, 두개의 영역에 대한 불확실성은  \n","\n","H(Y|X) = 0.7489  \n","\n","엔트로피 값이 작아진 걸로 보아 분할했을 때, 성능이 더 좋습니다."]},{"cell_type":"code","metadata":{"id":"FXkY0QC3eX6h","colab_type":"code","outputId":"d462f5df-add8-4660-9960-f4143c1ba8db","executionInfo":{"status":"ok","timestamp":1588387986109,"user_tz":-540,"elapsed":899,"user":{"displayName":"Mike Pk","photoUrl":"","userId":"01708764291721906768"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["Hxy =  -0.5 * ((7/8) *np.log2(7/8) + (1/8)*np.log2(1/8))   + ((-0.5)* ((3/8)*np.log2(3/8) + (5/8)*np.log2(5/8)  ) )\n","Hxy"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7489992230622806"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"R7Ayxxw1e8gY","colab_type":"text"},"source":["## Infomation Gain\n","\n","순도가 증가/불확실성이 감소하는 걸 두고 정보이론에서는 정보획득(information gain)이라고 합니다. \n","\n","<img src = \"https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcSBHseWW3gA01Vt-CX_aCVr4QdJWckDkfKPne_F5SDCYI67Yc_z&usqp=CAU\" width=\"70%\">"]},{"cell_type":"markdown","metadata":{"id":"XE3ro4HKvmKd","colab_type":"text"},"source":["엔트로피에서 조건부 엔트로피 값을 뺀 값이 gain 입니다. 즉 information gain은 기존의 엔트로피에서 조건부 엔트로피를 뺐을 때, 값이 얼마나 차이가 나는지를 보는 것입니다. "]},{"cell_type":"markdown","metadata":{"id":"HrfEVg0hzE5V","colab_type":"text"},"source":["예제  \n","<img src=  \"http://i.imgur.com/tNBkISS.png\" width=\"50%\">"]},{"cell_type":"markdown","metadata":{"id":"ljJOgzCI4UPp","colab_type":"text"},"source":["<img src = \"http://i.imgur.com/XgIfBPX.png\" width=\"25%\">  \n","임의의 변수를 기준으로 데이터를 sort  \n"]},{"cell_type":"markdown","metadata":{"id":"zinDVwKs_iUf","colab_type":"text"},"source":["의사결정나무는 계산복잡성 대비 높은 예측 성능을 내는 것으로 정평이 나 있습니다. 아울러 변수 단위로 설명력을 지닌다는 강점을 가지고 있습니다. 다만 의사결정나무는 결정경계(decision boundary)가 데이터 축에 수직이어서 특정 데이터에만 잘 작동할 가능성이 높습니다. \n","\n","\n","(overfitting)"]},{"cell_type":"markdown","metadata":{"id":"bI-2TuRADZnQ","colab_type":"text"},"source":["# Random Forest\n","\n","random forest 모델은 더 많은 decision tree를 결합하여 만든 앙상블 모델입니다.   \n","앙상블은 여러 base 모델을 결합하여 모델들의 예측을 평균, 다수결 등을 통해 더욱 정확도가 높은 예측을 만들어 냅니다. random forest는 base모델을 dicision tree를 사용한 앙상블 모델입니다.  \n","앙상블 모델은 무작위 예측이나 base모델이 서로 독립적일 때 좋은 성능을 발휘합니다.  \n","\n","\n","앙상블 모형  \n","<img src = \"http://postfiles9.naver.net/MjAxOTAzMTVfNDkg/MDAxNTUyNTgzNzUzNDk2.oAMN5mYOqbjKXRfMc1R6BcV_ko20Tj-kwzZ5xZhrnE0g.3skSpOXOBbmCHnLDbmGmAMh0NP1e7hX5jBcU8ENSoOMg.PNG.qbxlvnf11/Ensemble-learning.png?type=w773\" >  \n","\n","앙상블 전에 사용되는 개별 모델들을 weak learner 라고 부릅니다.  \n","앙상블 기법은 두 가지로 나눌 수 있는데, 취합방법과 부스팅 방법입니다.   \n","  \n","\n","취합방법 : weak learner 들이 미리 정해져 있어서 이를 취합하는 것  \n","ex) 다수결 기반 모형, bagging & pasting, random forest \n","\n","부스팅 방법 :  weak learner들을 순차적으로 결합해 나가는 방법  \n","ex) ada boost, gradient boost \n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xBB7vEwXWt3l","colab_type":"text"},"source":["앙상블 모델의 오류율  \n","\n","ex)  \n","<img src = \"http://faculty.juniata.edu/rhodes/ml/images/ensembleclasscalc.jpg\">   \n","ε = 0.35  \n","base model = 25  \n","\n","\n","앙상블 오류율과 base 모델 오류율\n","\n","<img src = \"http://freesearch.pe.kr/wp-content/uploads/1/XGd4aXWOXq.jpg\" >  \n","\n","base 모델의 오류율이 0.5보다 낮을 땐, 앙상블 모델의 성능이 더 좋다.  \n"]},{"cell_type":"markdown","metadata":{"id":"sDSfeObg2-Za","colab_type":"text"},"source":["Bagging  \n","배깅과 페이스팅 모두 무작위로 훈련 데이터 셋을 잘게 나눈 후 나누어진 훈련 데이터 셋을 여러 개의 모델에 할당하여 학습시킵니다.  \n","중복을 허용하여 샘플링을 하면 배깅(bootstrap aggregating), 중복을 허용하지 않고 샘플링을 하면 페이스팅(pasting) 이라고 합니다.  \n","\n","bootstrap : sampling.  복원추출 + 데이터의 수만큼의 크기로 split  \n","aggregating : 복원추출된 sample을 합치는 방법. 분류와 예측문제에 따라 다른 방법을 사용한다.   \n"," "]},{"cell_type":"markdown","metadata":{"id":"K2fnHHQ556dT","colab_type":"text"},"source":["<img src= \"https://miro.medium.com/max/1962/0*o4PJEzJahQnNC0HE\" >"]},{"cell_type":"markdown","metadata":{"id":"S4ecAXTV6VrT","colab_type":"text"},"source":["<img src = \"https://www.researchgate.net/profile/Ricardo_Aler2/publication/314420047/figure/fig1/AS:670008669646861@1536753923654/The-Bagging-Algorithm.png\" >  \n","regression 이라면 평균  \n","classification 이라면 voting  "]},{"cell_type":"markdown","metadata":{"id":"GrSOS6_0COBi","colab_type":"text"},"source":["random subspace  \n","\n","decision tree의 분기점을 선택할 때, 원래의 변수의 수보다 적은 임의의 변수를 선택해서 해당 변수만 고려하는 방법입니다.  \n"]},{"cell_type":"markdown","metadata":{"id":"30eTYcLiIcqd","colab_type":"text"},"source":["Hyper Parameter  \n","\n","radom subspace 할 무작위 변수의 수 : max_features (아마도?)    \n","- Classification : sqrt(변수)\n","- Regression : 변수 / 3 \n","\n","\n","base decision tree 개수:  n_estimators  \n","- 보통 2000개 이상을 사용한다고 한다.  \n"]},{"cell_type":"markdown","metadata":{"id":"pqnQi7fOTyM3","colab_type":"text"},"source":["Reference  \n","\n","decision tree  \n","https://ratsgo.github.io/machine%20learning/2017/03/26/tree/  \n","\n","random forest   \n","http://blog.naver.com/PostView.nhn?blogId=qbxlvnf11&logNo=221488622777&categoryNo=0&parentCategoryNo=0&viewDate=&currentPage=1&postListTopCurrentPage=1&from=postView \n","\n","\n","http://freesearch.pe.kr/archives/1071  \n","\n","https://www.youtube.com/watch?v=lIT5-piVtRw\n","\n"]}]}